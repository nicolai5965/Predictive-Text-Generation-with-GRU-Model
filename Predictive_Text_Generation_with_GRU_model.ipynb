{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolai5965/Predictive-Text-Generation-with-GRU-Model/blob/main/Predictive_Text_Generation_with_GRU_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wsYlOd58kKT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import words\n",
        "\n",
        "# Download the words corpus if you haven't done so already\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Dense, Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "# Mount Google Drive to load the dataset\n",
        "drive.mount('/content/drive') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpRhPB2sirBK"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0915PQSKisxL"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zszVsFGO8tua"
      },
      "outputs": [],
      "source": [
        "# Define the file path and load the train and test data\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Machine Learning/TensorFlow/GRU/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_mwPutS86lU"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(f'{filepath}fake_or_real_news.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Rb_jzt68-Nk"
      },
      "outputs": [],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_text_data(df, column_name, data_proportion, test_size, fixed_sequence_len, fixed_n_gram_len):\n",
        "    # Extract text data\n",
        "    text = list(df[column_name].values)\n",
        "\n",
        "    # Determine the number of samples to use\n",
        "    num_samples = int(len(text) * data_proportion)\n",
        "\n",
        "    # Use only the specified proportion of samples\n",
        "    text = text[:num_samples]\n",
        "\n",
        "    # Lowercasing: Convert all text to lowercase.\n",
        "    text = [t.lower() for t in text]\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    train_text, val_text = train_test_split(text, test_size=test_size, random_state=42)\n",
        "\n",
        "    # Define tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "\n",
        "    # Fit tokenizer on training text data\n",
        "    tokenizer.fit_on_texts(train_text)\n",
        "\n",
        "    # Total number of words before removal\n",
        "    total_words_before_removal = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Remove words that are not recognized as English words\n",
        "    english_words = set(words.words())\n",
        "    tokenizer.word_index = {word: index for word, index in tokenizer.word_index.items() if word in english_words}\n",
        "    removed_words = list(set(tokenizer.word_index.keys()) - english_words)\n",
        "\n",
        "    # Update total_words after removal\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Function to convert text data to sequence of tokens\n",
        "    def text_to_sequences(text_data):\n",
        "        input_sequences = []\n",
        "        for line in text_data:\n",
        "            token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "            for i in range(0, len(token_list) - fixed_n_gram_len + 1):\n",
        "                n_gram_sequence = token_list[i:i + fixed_n_gram_len]\n",
        "                input_sequences.append(n_gram_sequence)\n",
        "        return input_sequences\n",
        "\n",
        "    # Convert training and validation text data to sequences\n",
        "    train_sequences = text_to_sequences(train_text)\n",
        "    val_sequences = text_to_sequences(val_text)\n",
        "\n",
        "    # Pad sequences\n",
        "    train_sequences = np.array(pad_sequences(train_sequences, maxlen=fixed_sequence_len, padding='pre'))\n",
        "    val_sequences = np.array(pad_sequences(val_sequences, maxlen=fixed_sequence_len, padding='pre'))\n",
        "\n",
        "    # Create predictors and labels for both training and validation sets\n",
        "    train_x, train_y = train_sequences[:, :-1], train_sequences[:, -1]\n",
        "    val_x, val_y = val_sequences[:, :-1], val_sequences[:, -1]\n",
        "\n",
        "    # Convert to numpy array and set out-of-range indices to 0\n",
        "    train_y, val_y = np.array(train_y), np.array(val_y)\n",
        "    train_y[train_y >= total_words], val_y[val_y >= total_words] = 0, 0\n",
        "\n",
        "    # Convert labels to categorical\n",
        "    train_y = tf.keras.utils.to_categorical(train_y, num_classes=total_words)\n",
        "    val_y = tf.keras.utils.to_categorical(val_y, num_classes=total_words)\n",
        "\n",
        "    return (train_x, train_y), (val_x, val_y), total_words, tokenizer, removed_words"
      ],
      "metadata": {
        "id": "C13Ennlf9rX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3cCmu67XyHL"
      },
      "outputs": [],
      "source": [
        "# Custom GRU cell\n",
        "class GRUCell(Layer):\n",
        "    def __init__(self, units, activation='tanh', kernel_regularizer=None, recurrent_regularizer=None, **kwargs):\n",
        "        super(GRUCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel_z = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                        initializer='glorot_uniform',\n",
        "                                        name='kernel_z',\n",
        "                                        regularizer=self.kernel_regularizer)\n",
        "        self.recurrent_kernel_z = self.add_weight(shape=(self.units, self.units),\n",
        "                                                  initializer='orthogonal',\n",
        "                                                  name='recurrent_kernel_z',\n",
        "                                                  regularizer=self.recurrent_regularizer)\n",
        "        self.bias_z = self.add_weight(shape=(self.units,), initializer='zeros', name='bias_z')\n",
        "\n",
        "        self.kernel_r = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                        initializer='glorot_uniform',\n",
        "                                        name='kernel_r',\n",
        "                                        regularizer=self.kernel_regularizer)\n",
        "        self.recurrent_kernel_r = self.add_weight(shape=(self.units, self.units),\n",
        "                                                  initializer='orthogonal',\n",
        "                                                  name='recurrent_kernel_r',\n",
        "                                                  regularizer=self.recurrent_regularizer)\n",
        "        self.bias_r = self.add_weight(shape=(self.units,), initializer='zeros', name='bias_r')\n",
        "\n",
        "        self.kernel_h = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                        initializer='glorot_uniform',\n",
        "                                        name='kernel_h',\n",
        "                                        regularizer=self.kernel_regularizer)\n",
        "        self.recurrent_kernel_h = self.add_weight(shape=(self.units, self.units),\n",
        "                                                  initializer='orthogonal',\n",
        "                                                  name='recurrent_kernel_h',\n",
        "                                                  regularizer=self.recurrent_regularizer)\n",
        "        self.bias_h = self.add_weight(shape=(self.units,), initializer='zeros', name='bias_h')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        prev_output = states[0]\n",
        "\n",
        "        # Update gate calculation\n",
        "        z = self.update_gate(inputs, prev_output)\n",
        "\n",
        "        # Reset gate calculation\n",
        "        r = self.reset_gate(inputs, prev_output)\n",
        "\n",
        "        # Hidden state candidate\n",
        "        h_candidate = self.hidden_state_candidate(inputs, prev_output, r)\n",
        "\n",
        "        # Final output calculation\n",
        "        output = self.final_output(prev_output, h_candidate, z)\n",
        "\n",
        "        return output, [output]\n",
        "\n",
        "    def update_gate(self, inputs, prev_output):\n",
        "        return tf.keras.activations.sigmoid(tf.matmul(inputs, self.kernel_z) + tf.matmul(prev_output, self.recurrent_kernel_z) + self.bias_z)\n",
        "\n",
        "    def reset_gate(self, inputs, prev_output):\n",
        "        return tf.keras.activations.sigmoid(tf.matmul(inputs, self.kernel_r) + tf.matmul(prev_output, self.recurrent_kernel_r) + self.bias_r)\n",
        "\n",
        "    def hidden_state_candidate(self, inputs, prev_output, r):\n",
        "        return self.activation(tf.matmul(inputs, self.kernel_h) + tf.matmul(r * prev_output, self.recurrent_kernel_h) + self.bias_h)\n",
        "\n",
        "    def final_output(self, prev_output, h_candidate, z):\n",
        "        return (1 - z) * prev_output + z * h_candidate\n",
        "\n",
        "    def get_initial_state(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        return [tf.zeros([batch_size, self.units], dtype=inputs.dtype)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23K3cQLe-iNs"
      },
      "outputs": [],
      "source": [
        "# Custom GRU layer\n",
        "class GRU(Layer):\n",
        "    def __init__(self, cell, return_sequences=False, **kwargs):\n",
        "        super(GRU, self).__init__(**kwargs)\n",
        "        self.cell = cell\n",
        "        self.return_sequences = return_sequences\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Initialize the state\n",
        "        initial_state = self.cell.get_initial_state(inputs)\n",
        "        states = initial_state\n",
        "\n",
        "        # Process the input sequence\n",
        "        outputs = []\n",
        "        for t in range(inputs.shape[1]):\n",
        "            input_t = inputs[:, t]\n",
        "            output, states = self.cell(input_t, states)\n",
        "            outputs.append(output)\n",
        "\n",
        "        # Stack the outputs\n",
        "        outputs = tf.stack(outputs, axis=1)\n",
        "\n",
        "        if self.return_sequences:\n",
        "            return outputs\n",
        "        else:\n",
        "            return outputs[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aamvUF0s-gc7"
      },
      "outputs": [],
      "source": [
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, hidden_units, max_features, l2_reg):\n",
        "        super(Model, self).__init__()\n",
        "        self.embedding = Embedding(input_dim=max_features, output_dim=embedding_dim)\n",
        "        self.gru_cell = GRUCell(units=hidden_units, kernel_regularizer=regularizers.l2(l2_reg), recurrent_regularizer=regularizers.l2(l2_reg))\n",
        "        self.gru = GRU(self.gru_cell, return_sequences=False)\n",
        "        self.dense = Dense(max_features, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.gru(x)\n",
        "        return self.dense(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing configuration\n",
        "preprocessing_config = {\n",
        "    'data_proportion': 0.04,\n",
        "    'text_split_size': 0.2,\n",
        "    'fixed_sequence_len': 25,\n",
        "    'fixed_n_gram_len': 20\n",
        "}\n"
      ],
      "metadata": {
        "id": "tSGyrQ0DsBv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the text data\n",
        "(train_x, train_y), (val_x, val_y), total_words, tokenizer, removed_words = prepare_text_data(df, \n",
        "                                                                    'text', \n",
        "                                                                    preprocessing_config['data_proportion'], \n",
        "                                                                    preprocessing_config['text_split_size'],\n",
        "                                                                    preprocessing_config['fixed_sequence_len'], \n",
        "                                                                    preprocessing_config['fixed_n_gram_len'])"
      ],
      "metadata": {
        "id": "Q566XdaysQCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display shape of predictors and label\n",
        "print(\"Train Predictors shape:\", train_x.shape)\n",
        "print(\"Train Label shape:\", train_y.shape)\n",
        "print(\"Validation Predictors shape:\", val_x.shape)\n",
        "print(\"Validation Label shape:\", val_y.shape)\n",
        "\n",
        "# Display an example predictor and label\n",
        "print(\"Example Predictor:\", train_x[0])\n",
        "print(\"Example Label:\", train_y[0])\n",
        "\n",
        "# Display the total number of unique words\n",
        "print(\"Total number of unique words:\", total_words)\n",
        "\n",
        "# Output removed words\n",
        "if len(removed_words) > 0:\n",
        "    print(\"Number of words removed:\", len(removed_words))\n",
        "    print(\"Removed words:\", removed_words)\n",
        "else:\n",
        "    print(\"No words were removed.\")"
      ],
      "metadata": {
        "id": "cOpj0bBJsMF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display distribution of sequence lengths\n",
        "sequence_lengths = [len(seq) for seq in np.concatenate([train_x, val_x])]\n",
        "plt.hist(sequence_lengths, bins=30)\n",
        "plt.title('Distribution of Sequence Lengths')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Get word counts\n",
        "word_counts = tokenizer.word_counts\n",
        "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display most common and least common words\n",
        "print(\"10 most common words:\", sorted_word_counts[:10])\n",
        "print(\"10 least common words:\", sorted_word_counts[-10:])\n",
        "\n",
        "# Count and display number of words that appear only once\n",
        "single_appearance_words = len([count for word, count in word_counts.items() if count == 1])\n",
        "print(\"Number of words that appear only once:\", single_appearance_words)\n",
        "\n",
        "# Display example of original text and its tokenized equivalent\n",
        "example_text = df['text'][0]  # As the original text isn't directly returned by the function anymore\n",
        "example_tokens = tokenizer.texts_to_sequences([example_text])[0]\n",
        "print(\"Example original text:\", example_text[:20])\n",
        "print(\"Example tokenized text:\", example_tokens[:20])\n"
      ],
      "metadata": {
        "id": "h7GVRFrEuHxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration\n",
        "model_config = {\n",
        "    'embedding_dim': 100,\n",
        "    'hidden_units': 32,\n",
        "    'max_features': total_words,\n",
        "    'l2_reg': 1e-4,\n",
        "    'optimizer': 'RMSprop',\n",
        "    'learning_rate': 0.01,\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'metrics': ['accuracy'],\n",
        "    'batch_size': 256,\n",
        "    'epochs': 10,\n",
        "    'validation_split': 0.2,\n",
        "    'patience': 5,\n",
        "    'verbose': 2\n",
        "}"
      ],
      "metadata": {
        "id": "zzd10bFVINHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and compile the model\n",
        "model = Model(model_config['embedding_dim'], model_config['hidden_units'], model_config['max_features'], model_config['l2_reg'])\n",
        "model.compile(optimizer=RMSprop(learning_rate=model_config['learning_rate']), \n",
        "              loss=model_config['loss'], \n",
        "              metrics=model_config['metrics'])\n",
        "\n",
        "# Define the early stopping callback with the desired parameters\n",
        "early_stopping = EarlyStopping(monitor='val_loss', \n",
        "                               patience=model_config['patience'], \n",
        "                               restore_best_weights=True)\n",
        "\n",
        "# Specify the path where you want to save the model\n",
        "checkpoint_filepath = '/content/drive/My Drive/Colab Notebooks/Machine Learning/TensorFlow/GRU/GRU_best_model_callback/'\n",
        "\n",
        "# Create a ModelCheckpoint callback that saves the weights only during training\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n"
      ],
      "metadata": {
        "id": "jr6c-xQ1sFC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(train_x, \n",
        "                    train_y, \n",
        "                    epochs=model_config['epochs'], \n",
        "                    batch_size=model_config['batch_size'], \n",
        "                    validation_data=(val_x, val_y), \n",
        "                    callbacks=[early_stopping, model_checkpoint_callback], \n",
        "                    verbose=model_config['verbose'])"
      ],
      "metadata": {
        "id": "c3Yf7YYhwjNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(model.summary())"
      ],
      "metadata": {
        "id": "uRwufawOAxbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAJo31sWHAQv"
      },
      "outputs": [],
      "source": [
        "def predict_next_word(model, tokenizer, input_text, fixed_sequence_len, n_best=3):\n",
        "    # Tokenize the input text\n",
        "    input_tokens = tokenizer.texts_to_sequences([input_text])[0]\n",
        "    # Pad the tokens\n",
        "    input_tokens = pad_sequences([input_tokens], maxlen=fixed_sequence_len-1, padding='pre')\n",
        "    # Predict the probabilities for the next word\n",
        "    probabilities = model.predict(input_tokens, verbose=0)\n",
        "    # Get the indices of the n_best most probable next words\n",
        "    top_indices = np.argpartition(probabilities[0], -n_best)[-n_best:]\n",
        "    # Get these indices sorted by probability\n",
        "    sorted_indices = top_indices[np.argsort(-probabilities[0][top_indices])]\n",
        "    # Reverse the word index\n",
        "    reverse_word_index = dict([(value, key) for (key, value) in tokenizer.word_index.items()])\n",
        "    # Filter valid indices\n",
        "    valid_indices = [i for i in sorted_indices if i in reverse_word_index and i != 0]  # <-- Modified line\n",
        "    # Look up the words corresponding to the valid indices\n",
        "    next_words = [reverse_word_index[i] for i in valid_indices]  # <-- Modified line\n",
        "    # Look up the probabilities corresponding to the valid indices\n",
        "    likelihoods = probabilities[0][valid_indices]  # <-- Modified line\n",
        "    # Create a DataFrame with the words and their likelihoods\n",
        "    df = pd.DataFrame({\n",
        "        'Word': next_words,\n",
        "        'Likelihood': [f\"{likelihood * 100:.2f}%\" for likelihood in likelihoods]\n",
        "    })\n",
        "    return df\n",
        "\n",
        "best_words = 3\n",
        "\n",
        "possible = predict_next_word(model, tokenizer, \"The economic outlook for the next year has been released and it suggests that the growth rate will\", preprocessing_config['fixed_sequence_len'])\n",
        "display(possible)\n",
        "possible = predict_next_word(model, tokenizer, \"In the world of artificial intelligence, recent advancements have led to a renewed interest in developing systems that can\", preprocessing_config['fixed_sequence_len'])\n",
        "display(possible)\n",
        "possible = predict_next_word(model, tokenizer, \"Climate change is having a dramatic effect on global weather patterns, and scientists are warning that if we don't\", preprocessing_config['fixed_sequence_len'])\n",
        "display(possible)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, input_text, fixed_sequence_len, n_words, creativity=3):\n",
        "    word_sequence = input_text.split()\n",
        "    for _ in range(n_words):\n",
        "        sub_sequence = \" \".join(word_sequence[-fixed_sequence_len:])\n",
        "        try:\n",
        "            predictions = predict_next_word(model, tokenizer, sub_sequence, fixed_sequence_len, n_best=creativity)\n",
        "            predictions.sort_values(by='Likelihood', ascending=False, inplace=True)\n",
        "            top_predictions = predictions.head(creativity)['Word'].tolist()\n",
        "            choice = random.choice(top_predictions)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during prediction: {e}\")\n",
        "            choice = random.choice(word_sequence)\n",
        "        word_sequence.append(choice)\n",
        "    return \" \".join(word_sequence)\n",
        "\n",
        "\n",
        "input_string = \"The economic outlook for the next year has been released and it suggests that the growth rate will\"\n",
        "generated_text = generate_text(model, tokenizer, input_string, preprocessing_config['fixed_sequence_len'], n_words=100, creativity=3)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "uw_C4jKzCZQg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "authorship_tag": "ABX9TyPWbAhYcs41YzUuYYDi45xu",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}